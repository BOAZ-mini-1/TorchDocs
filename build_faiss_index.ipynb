{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==== CONFIG ====\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR  = Path(\"/content/drive/MyDrive/Faiss/sentence_transformers_all_MiniLM_L6_v2\")  # 경로 설정\n",
        "EMB_DIR   = BASE_DIR / \"embeddings\"      # 원본(.npy/.json)\n",
        "INDEX_DIR = BASE_DIR / \"index\"           # 산출물 저장 위치\n",
        "OVERWRITE = False                        # True면 기존 산출물 덮어씀\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2lTlrHPkkem",
        "outputId": "ad35c742-e29e-4a12-95e6-ae54ea8e4dad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 원본 embeddings 중복 검토"
      ],
      "metadata": {
        "id": "a8ddUquXvWNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CONFIG (네 설정 사용) ====\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "from collections import Counter, defaultdict\n",
        "import itertools\n",
        "\n",
        "BASE_DIR  = Path(\"/content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0\") # 경로 설정\n",
        "EMB_DIR   = BASE_DIR / \"embeddings\"      # 원본(.npy/.json)\n",
        "\n",
        "# 스캔 대상: id_*_BAAI_bge_base_en_cls(.json 포함 가능)\n",
        "cand_files = sorted(EMB_DIR.glob(\"id_*_Snowflake_snowflake_arctic_embed_l_v2.0_cls*\")) # 모델 이름에 맞게 입력 (버전 다음 부분부터 _cls/_mean까지)\n",
        "\n",
        "print(\"=== 스캔 대상 파일 ===\")\n",
        "if not cand_files:\n",
        "    print(\"(!) 대상 파일을 찾지 못했습니다. 경로/파일명을 확인하세요.\")\n",
        "for p in cand_files:\n",
        "    print(\"-\", p.name)\n",
        "\n",
        "# 예: id_2.4_BAAI_bge_base_en_cls[.json] 에서 2.4만 뽑기\n",
        "version_rx = re.compile(r\"id_(?P<ver>\\d+\\.\\d+)_Snowflake_snowflake_arctic_embed_l_v2.0_cls\", re.IGNORECASE)\n",
        "\n",
        "# 수집 컨테이너\n",
        "all_rows = []     # (id, file, index, version)\n",
        "per_file_stats = []\n",
        "per_file_dup_examples = {}  # {file: {id: [indices,...]}}\n",
        "\n",
        "# ==== 파일별 검사 (내부 중복 / index 연속성) ====\n",
        "for path in cand_files:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    m = version_rx.search(path.name)\n",
        "    version = m.group(\"ver\") if m else \"unknown\"\n",
        "\n",
        "    idmap = data.get(\"id_mapping\", [])\n",
        "    ids   = [row[\"id\"] for row in idmap]\n",
        "    idxs  = [row[\"index\"] for row in idmap]\n",
        "\n",
        "    # 내부 중복(id)\n",
        "    cnt = Counter(ids)\n",
        "    dup_ids = [k for k, c in cnt.items() if c > 1]\n",
        "\n",
        "    # 중복 id의 인덱스들 예시 수집\n",
        "    dup_detail = {}\n",
        "    if dup_ids:\n",
        "        pos = defaultdict(list)\n",
        "        for row in idmap:\n",
        "            pos[row[\"id\"]].append(row[\"index\"])\n",
        "        for _id in dup_ids:\n",
        "            dup_detail[_id] = sorted(pos[_id])\n",
        "        per_file_dup_examples[path.name] = dup_detail\n",
        "\n",
        "    # 인덱스 연속성(0..N-1) & 중복 여부\n",
        "    idx_set = set(idxs)\n",
        "    idx_is_unique = (len(idx_set) == len(idxs))\n",
        "    idx_contiguous = (idx_is_unique and len(idx_set) > 0 and min(idx_set) == 0 and max(idx_set) == len(idx_set)-1)\n",
        "\n",
        "    per_file_stats.append({\n",
        "        \"file\": path.name,\n",
        "        \"version\": version,\n",
        "        \"items\": len(idmap),\n",
        "        \"unique_ids\": len(cnt),\n",
        "        \"dup_ids_in_file\": len(dup_ids),\n",
        "        \"index_unique\": idx_is_unique,\n",
        "        \"index_contiguous_0_to_n_1\": idx_contiguous,\n",
        "    })\n",
        "\n",
        "    # 전체 행 수집(파일 간 중복 검사용)\n",
        "    for row in idmap:\n",
        "        all_rows.append((row[\"id\"], path.name, row[\"index\"], version))\n",
        "\n",
        "# ==== 파일별 요약 출력 ====\n",
        "print(\"\\n=== 파일별 기본 통계 ===\")\n",
        "for s in per_file_stats:\n",
        "    print(f\"{s['file']} | ver={s['version']} | items={s['items']:,} | unique_ids={s['unique_ids']:,} \"\n",
        "          f\"| dup_in_file={s['dup_ids_in_file']:,} | idx_unique={s['index_unique']} | idx_contiguous={s['index_contiguous_0_to_n_1']}\")\n",
        "\n",
        "# 내부 중복 상세 예시 출력\n",
        "print(\"\\n=== (옵션) 내부 중복 상세 예시 (파일별 상위 최대 5개 id) ===\")\n",
        "if not per_file_dup_examples:\n",
        "    print(\"내부 중복 id 없음.\")\n",
        "else:\n",
        "    for fname, dupdict in per_file_dup_examples.items():\n",
        "        print(f\"\\n[파일] {fname}  (중복 id 개수: {len(dupdict)})\")\n",
        "        for _id, positions in itertools.islice(dupdict.items(), 5):\n",
        "            print(f\"- id={_id} -> indices={positions}\")\n",
        "\n",
        "# ==== 파일 간 중복(id) ====\n",
        "by_id = defaultdict(list)\n",
        "for _id, fname, idx, ver in all_rows:\n",
        "    by_id[_id].append((fname, idx, ver))\n",
        "\n",
        "cross_file_dups = {}  # id -> [(file, idx, ver), ...] (두 개 이상 다른 파일에 등장)\n",
        "for _id, occ in by_id.items():\n",
        "    files = {x[0] for x in occ}\n",
        "    if len(files) > 1:\n",
        "        cross_file_dups[_id] = occ\n",
        "\n",
        "print(\"\\n=== 파일 간 중복 요약 ===\")\n",
        "total_distinct_ids = len(by_id)\n",
        "num_cross_dup_ids = len(cross_file_dups)\n",
        "print(f\"- 서로 다른 ID 총계: {total_distinct_ids:,}\")\n",
        "print(f\"- 파일 간 중복 ID 개수: {num_cross_dup_ids:,}\")\n",
        "if num_cross_dup_ids == 0:\n",
        "    print(\"파일 간 중복 없음.\")\n",
        "\n",
        "# 파일 간 중복 예시 출력\n",
        "if num_cross_dup_ids > 0:\n",
        "    print(\"\\n=== 파일 간 중복 상세 예시 (상위 10개) ===\")\n",
        "    shown = 0\n",
        "    for _id, occ in cross_file_dups.items():\n",
        "        files = sorted({o[0] for o in occ})\n",
        "        print(f\"\\n[id] { _id }  (등장 파일 수: {len(files)})\")\n",
        "        print(\"  files:\", \", \".join(files))\n",
        "        # 각 발생지 최대 5줄 예시\n",
        "        for f, i, v in itertools.islice(sorted(occ, key=lambda x: (x[0], x[1])), 5):\n",
        "            print(f\"  - {f} | index={i} | ver={v}\")\n",
        "        shown += 1\n",
        "        if shown >= 10:\n",
        "            break\n",
        "\n",
        "# ==== 인덱스 불연속/중복 파일만 골라 보여주기 ====\n",
        "bad_index_files = [s for s in per_file_stats if not s[\"index_contiguous_0_to_n_1\"]]\n",
        "print(\"\\n=== 인덱스 불연속/중복 의심 파일 ===\")\n",
        "if not bad_index_files:\n",
        "    print(\"없음 (모든 파일의 index가 0..N-1 연속이고 중복 없음).\")\n",
        "else:\n",
        "    for s in bad_index_files:\n",
        "        print(f\"- {s['file']} | items={s['items']:,} | idx_unique={s['index_unique']} | idx_contiguous={s['index_contiguous_0_to_n_1']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Walic5x9k6d-",
        "outputId": "95608729-b969-4f3a-8f22-085fa77da7d6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 스캔 대상 파일 ===\n",
            "- id_2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json\n",
            "- id_2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json\n",
            "- id_2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json\n",
            "- id_2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json\n",
            "- id_2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json\n",
            "\n",
            "=== 파일별 기본 통계 ===\n",
            "id_2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json | ver=2.4 | items=7,902 | unique_ids=7,899 | dup_in_file=3 | idx_unique=True | idx_contiguous=True\n",
            "id_2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json | ver=2.5 | items=8,122 | unique_ids=8,119 | dup_in_file=3 | idx_unique=True | idx_contiguous=True\n",
            "id_2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json | ver=2.6 | items=8,093 | unique_ids=8,090 | dup_in_file=3 | idx_unique=True | idx_contiguous=True\n",
            "id_2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json | ver=2.7 | items=8,203 | unique_ids=8,200 | dup_in_file=3 | idx_unique=True | idx_contiguous=True\n",
            "id_2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json | ver=2.8 | items=6,410 | unique_ids=6,410 | dup_in_file=0 | idx_unique=True | idx_contiguous=True\n",
            "\n",
            "=== (옵션) 내부 중복 상세 예시 (파일별 상위 최대 5개 id) ===\n",
            "\n",
            "[파일] id_2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json  (중복 id 개수: 3)\n",
            "- id=300396de51e1f1034ce64f4e3a36b903f43bb985 -> indices=[113, 117]\n",
            "- id=8907328a9ef69bc453f3f55e3fd90bd461e14678 -> indices=[7451, 7485]\n",
            "- id=f18b3bccedd55f8221497387b64325c2ebf306ed -> indices=[7576, 7578]\n",
            "\n",
            "[파일] id_2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json  (중복 id 개수: 3)\n",
            "- id=be917dc4de73f230d926d09a98c776060e88f966 -> indices=[116, 120]\n",
            "- id=2fcb44013a9391f52b9d6eaff0f66527d45a6630 -> indices=[7661, 7695]\n",
            "- id=9e8805ba5e4bfa5839b81a278c477b39fe2f805b -> indices=[7787, 7789]\n",
            "\n",
            "[파일] id_2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json  (중복 id 개수: 3)\n",
            "- id=3a3aa7be6555f6185e98d8866d8a6ca93d0defd0 -> indices=[119, 123]\n",
            "- id=06acf8db910e9ce4e1586b73ffb04462124bcc56 -> indices=[7723, 7757]\n",
            "- id=de86aa2f129c088e65b6cc62bdebe235fe6e27ef -> indices=[7846, 7848]\n",
            "\n",
            "[파일] id_2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls.json  (중복 id 개수: 3)\n",
            "- id=83e92af1330a3767c1f4c9f6c3f4299c27298ceb -> indices=[120, 124]\n",
            "- id=27f0699f4edbf4360cb5254e6522db68e17e3a83 -> indices=[7833, 7867]\n",
            "- id=ee7e6e38ff3b68d02a8adbd7ad8631bd8c34c136 -> indices=[7956, 7958]\n",
            "\n",
            "=== 파일 간 중복 요약 ===\n",
            "- 서로 다른 ID 총계: 38,718\n",
            "- 파일 간 중복 ID 개수: 0\n",
            "파일 간 중복 없음.\n",
            "\n",
            "=== 인덱스 불연속/중복 의심 파일 ===\n",
            "없음 (모든 파일의 index가 0..N-1 연속이고 중복 없음).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- index 생성"
      ],
      "metadata": {
        "id": "GDTrxHO9v9CU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W8k3cfz6J6U",
        "outputId": "b1da7aac-106d-4d8d-ecc6-8da6e03890e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 완료\n",
            " - faiss.index : /content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0/index/faiss.index\n",
            " - stats.json  : /content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0/index/stats.json\n",
            " - global_ids  : /content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0/index/global_ids.json (38730)\n",
            " - ntotal/dim  : 38730 / 1024\n",
            " - datasets    : ['2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls', '2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls', '2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls', '2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls', '2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls']\n",
            " - version_ranges : [{'version': '2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls', 'start': 0, 'end': 7901}, {'version': '2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls', 'start': 7902, 'end': 16023}, {'version': '2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls', 'start': 16024, 'end': 24116}] ... (총 5 )\n"
          ]
        }
      ],
      "source": [
        "# ==== CONFIG ====\n",
        "from pathlib import Path\n",
        "BASE_DIR   = Path(\"/content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0\")  # 경로 설정\n",
        "EMB_DIR    = BASE_DIR / \"embeddings\"\n",
        "INDEX_DIR  = BASE_DIR / \"index\"\n",
        "OVERWRITE  = False\n",
        "\n",
        "# 이 줄만 모델별로 바꿔 쓰면 됨\n",
        "MODEL_NAME = \"Snowflake_snowflake_arctic_embed_l_v2.0_cls\"   # _cls/_mean까지\n",
        "\n",
        "# ==== IMPORT (faiss 자동 설치) ====\n",
        "try:\n",
        "    import faiss  # noqa\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\"], check=True)\n",
        "    import faiss\n",
        "\n",
        "import os, glob, json, numpy as np\n",
        "\n",
        "# ==== FUNCTIONS ====\n",
        "def ensure_dirs():\n",
        "    INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def key_of(path_str: str) -> str:\n",
        "    \"\"\"\n",
        "    현재 파일명 규칙:\n",
        "      - embeddings_<ver>_<MODEL_NAME>.npy\n",
        "      - id_<ver>_<MODEL_NAME>           (확장자 없음 가능)\n",
        "      - metadata_<ver>_<MODEL_NAME>.parquet\n",
        "    페어링 키: '<ver>_<MODEL_NAME>'\n",
        "    \"\"\"\n",
        "    name = os.path.basename(path_str)\n",
        "    for prefix in (\"embeddings_\", \"id_\", \"metadata_\"):\n",
        "        sig = f\"_{MODEL_NAME}\"\n",
        "        if name.startswith(prefix) and sig in name:\n",
        "            core = name[len(prefix):]\n",
        "            core = core.replace(\".npy\", \"\").replace(\".json\", \"\").replace(\".parquet\", \"\")\n",
        "            return core\n",
        "    return name\n",
        "\n",
        "def find_pairs(emb_dir: Path):\n",
        "    emb_glob = f\"embeddings_*_{MODEL_NAME}.npy\"\n",
        "    id_json_glob = f\"id_*_{MODEL_NAME}.json\"\n",
        "    id_nox_glob  = f\"id_*_{MODEL_NAME}\"        # 확장자 없는 케이스\n",
        "\n",
        "    emb_paths = sorted(glob.glob(str(emb_dir / emb_glob)))\n",
        "    map_paths = sorted(set(\n",
        "        glob.glob(str(emb_dir / id_json_glob)) +\n",
        "        glob.glob(str(emb_dir / id_nox_glob))\n",
        "    ))\n",
        "\n",
        "    emb_by_key = {key_of(p): p for p in emb_paths}\n",
        "    map_by_key = {key_of(p): p for p in map_paths}\n",
        "    keys = sorted(set(emb_by_key) & set(map_by_key))\n",
        "    if not keys:\n",
        "        raise RuntimeError(\n",
        "            \"페어링 실패: 파일명을 확인하세요.\\n\"\n",
        "            f\"- embeddings 패턴: {emb_glob}\\n\"\n",
        "            f\"- id 패턴: {id_json_glob} / {id_nox_glob}\\n\"\n",
        "            f\"- EMB_DIR: {emb_dir}\\n\"\n",
        "        )\n",
        "    return emb_by_key, map_by_key, keys\n",
        "\n",
        "def _load_ids(map_path: str):\n",
        "    with open(map_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    id_list = data[\"id_mapping\"] if isinstance(data, dict) and \"id_mapping\" in data else data\n",
        "    if not isinstance(id_list, list) or not id_list or not isinstance(id_list[0], dict) or \"id\" not in id_list[0]:\n",
        "        raise ValueError(f\"[형식 오류] {map_path} : 'id' 키를 가진 dict 리스트가 필요합니다.\")\n",
        "    return id_list\n",
        "\n",
        "def validate_pairs(emb_by_key, map_by_key, keys):\n",
        "    total_rows, dim = 0, None\n",
        "    for k in keys:\n",
        "        Xmm = np.load(emb_by_key[k], mmap_mode=\"r\")\n",
        "        rows, d = Xmm.shape\n",
        "        ids = _load_ids(map_by_key[k])\n",
        "        if rows != len(ids):\n",
        "            raise ValueError(f\"[행수 불일치] {k}: npy={rows} vs ids={len(ids)}\")\n",
        "        if dim is None:\n",
        "            dim = d\n",
        "        elif d != dim:\n",
        "            raise ValueError(f\"[차원 불일치] {k}: 기대 {dim}, 실제 {d}\")\n",
        "        total_rows += rows\n",
        "    return {\"total_rows\": total_rows, \"dimension\": dim}\n",
        "\n",
        "def build_faiss_index(emb_by_key, keys, index_dir: Path, overwrite=False):\n",
        "    index_path = index_dir / \"faiss.index\"\n",
        "    if index_path.exists() and not overwrite:\n",
        "        return str(index_path), False\n",
        "\n",
        "    vecs_list, datasets, dim = [], {}, None\n",
        "    for k in keys:\n",
        "        X = np.load(emb_by_key[k]).astype(\"float32\")\n",
        "        vecs_list.append(X)\n",
        "        datasets[k] = {\"rows\": int(X.shape[0]), \"dim\": int(X.shape[1])}\n",
        "        if dim is None:\n",
        "            dim = X.shape[1]\n",
        "    X_all = np.concatenate(vecs_list, axis=0)\n",
        "\n",
        "    faiss.normalize_L2(X_all)\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(X_all)\n",
        "    faiss.write_index(index, str(index_path))\n",
        "    return str(index_path), datasets\n",
        "\n",
        "def build_global_ids_from_mappings(map_by_key: dict, keys: list, index_dir: Path):\n",
        "    out_path = Path(index_dir) / \"global_ids.json\"\n",
        "    global_ids = []\n",
        "    for k in keys:\n",
        "        for item in _load_ids(map_by_key[k]):\n",
        "            global_ids.append(item[\"id\"])\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"ids\": global_ids}, f, ensure_ascii=False, indent=2)\n",
        "    return str(out_path), len(global_ids)\n",
        "\n",
        "def write_stats(index_dir: Path, dimension: int, total_rows: int, datasets: dict, keys=None):\n",
        "    version_ranges, cursor = [], 0\n",
        "    if keys:\n",
        "        for k in keys:\n",
        "            rows = int(datasets[k][\"rows\"])\n",
        "            if rows <= 0:\n",
        "                continue\n",
        "            start, end = cursor, cursor + rows - 1\n",
        "            version_ranges.append({\"version\": str(k), \"start\": start, \"end\": end})\n",
        "            cursor += rows\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"intfloat/e5-large-v2\",  # 스키마 유지(원 코드와 키 동일)\n",
        "        \"pooling\": \"mean\",\n",
        "        \"metric\": \"cosine (IP on L2-normalized vectors)\",\n",
        "        \"dimension\": int(dimension),\n",
        "        \"total_rows\": int(total_rows),\n",
        "        \"datasets\": datasets,\n",
        "        \"version_ranges\": version_ranges\n",
        "    }\n",
        "    stats_path = Path(index_dir) / \"stats.json\"\n",
        "    with open(stats_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "    return str(stats_path)\n",
        "\n",
        "def verify(index_dir: Path):\n",
        "    idx = faiss.read_index(str(Path(index_dir) / \"faiss.index\"))\n",
        "    with open(Path(index_dir) / \"stats.json\", encoding=\"utf-8\") as f:\n",
        "        s = json.load(f)\n",
        "\n",
        "    assert idx.ntotal == s[\"total_rows\"], \"인덱스 행수와 stats 불일치\"\n",
        "    assert idx.d == s[\"dimension\"], \"인덱스 차원과 stats 불일치\"\n",
        "\n",
        "    with open(Path(index_dir) / \"global_ids.json\", encoding=\"utf-8\") as f:\n",
        "        gids = json.load(f)[\"ids\"]\n",
        "    assert len(gids) == s[\"total_rows\"], \"global_ids 개수와 stats 불일치\"\n",
        "\n",
        "    vr = s.get(\"version_ranges\", [])\n",
        "    if vr:\n",
        "        assert vr[-1][\"end\"] == len(gids) - 1, \"version_ranges의 end와 global_ids 길이가 맞지 않습니다.\"\n",
        "    return {\"ntotal\": idx.ntotal, \"dimension\": idx.d, \"datasets\": list(s[\"datasets\"].keys()), \"version_ranges\": vr}\n",
        "\n",
        "# ==== MAIN ====\n",
        "def main():\n",
        "    ensure_dirs()\n",
        "    emb_by_key, map_by_key, keys = find_pairs(EMB_DIR)\n",
        "    summary = validate_pairs(emb_by_key, map_by_key, keys)\n",
        "\n",
        "    index_path, datasets = build_faiss_index(emb_by_key, keys, INDEX_DIR, overwrite=OVERWRITE)\n",
        "    created = isinstance(datasets, dict)\n",
        "\n",
        "    if created:\n",
        "        stats_path = write_stats(INDEX_DIR, summary[\"dimension\"], summary[\"total_rows\"], datasets, keys)\n",
        "    else:\n",
        "        stats_path = str(Path(INDEX_DIR) / \"stats.json\")\n",
        "        if not Path(stats_path).exists():\n",
        "            raise RuntimeError(\"faiss.index는 존재하지만 stats.json이 없습니다. OVERWRITE=True 로 재생성하세요.\")\n",
        "\n",
        "    gid_path, gid_len = build_global_ids_from_mappings(map_by_key, keys, INDEX_DIR)\n",
        "\n",
        "    check = verify(INDEX_DIR)\n",
        "    print(\"✅ 완료\")\n",
        "    print(\" - faiss.index :\", index_path)\n",
        "    print(\" - stats.json  :\", stats_path)\n",
        "    print(\" - global_ids  :\", gid_path, f\"({gid_len})\")\n",
        "    print(\" - ntotal/dim  :\", check['ntotal'], \"/\", check['dimension'])\n",
        "    print(\" - datasets    :\", check['datasets'])\n",
        "    if check[\"version_ranges\"]:\n",
        "        print(\" - version_ranges :\", check[\"version_ranges\"][:3], \"... (총\", len(check[\"version_ranges\"]), \")\")\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- embeddings - index 매핑/중복 검사"
      ],
      "metadata": {
        "id": "sWwHi54xu-6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CONFIG (검증 전용: 모델명만 바꿔 쓰면 됨) ====\n",
        "from pathlib import Path\n",
        "BASE_DIR   = Path(\"/content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0\")  # 경로 설정\n",
        "EMB_DIR    = BASE_DIR / \"embeddings\"\n",
        "INDEX_DIR  = BASE_DIR / \"index\"\n",
        "MODEL_NAME = \"Snowflake_snowflake_arctic_embed_l_v2.0_cls\"  # _cls/_mean까지\n",
        "\n",
        "# ==== IMPORTS ====\n",
        "from collections import Counter\n",
        "import json, numpy as np, faiss, os, glob\n",
        "\n",
        "# === 1) stats.json / global_ids.json / faiss.index 정합성 확장검증 ===\n",
        "with open(INDEX_DIR / \"stats.json\", encoding=\"utf-8\") as f:\n",
        "    stats = json.load(f)\n",
        "\n",
        "with open(INDEX_DIR / \"global_ids.json\", encoding=\"utf-8\") as f:\n",
        "    gids = json.load(f)[\"ids\"]\n",
        "\n",
        "idx = faiss.read_index(str(INDEX_DIR / \"faiss.index\"))\n",
        "\n",
        "print(\"=== 기본 정합성 ===\")\n",
        "print(\"ntotal == total_rows :\", idx.ntotal == stats[\"total_rows\"], idx.ntotal, stats[\"total_rows\"])\n",
        "print(\"dim == stats.dimension:\", idx.d == stats[\"dimension\"], idx.d, stats[\"dimension\"])\n",
        "print(\"len(global_ids) == total_rows:\", len(gids) == stats[\"total_rows\"], len(gids), stats[\"total_rows\"])\n",
        "\n",
        "# === 2) version_ranges별 길이 == datasets.rows 일치 여부 ===\n",
        "print(\"\\n=== version_ranges vs datasets.rows ===\")\n",
        "vr_ok = True\n",
        "for vr in stats.get(\"version_ranges\", []):\n",
        "    version, s, e = vr[\"version\"], vr[\"start\"], vr[\"end\"]\n",
        "    expected = stats[\"datasets\"][version][\"rows\"]\n",
        "    actual = (e - s + 1)\n",
        "    ok = (expected == actual)\n",
        "    print(f\"{version:>35} | start={s:>6}, end={e:>6}, rows={expected:>6} | span={actual:>6} | ok={ok}\")\n",
        "    vr_ok = vr_ok and ok\n",
        "print(\"ALL_OK:\", vr_ok)\n",
        "\n",
        "# === 3) 샘플 검증: 각 버전의 앞/뒤 2개 글로벌 인덱스가 원본 id 순서와 일치하는지 ===\n",
        "def key_of(path_str: str) -> str:\n",
        "    \"\"\"버전 키 통합 파서: 옛 e5 형식과 새 통일형식 모두 대응.\"\"\"\n",
        "    name = os.path.basename(path_str)\n",
        "\n",
        "    # (A) 옛 e5 형식: id_mapping_torchdocs_<key>_intfloat...json\n",
        "    if name.startswith(\"id_mapping_torchdocs_\") and \"_intfloat\" in name:\n",
        "        return name.split(\"id_mapping_torchdocs_\")[1].split(\"_intfloat\")[0]\n",
        "    if name.startswith(\"id_mapping_torchdocs_\") and \"_mean\" in name:\n",
        "        return name.split(\"id_mapping_torchdocs_\")[1].split(\"_mean\")[0]\n",
        "\n",
        "    # (B) 새 통일 형식: id_<ver>_<MODEL_NAME>[.json]\n",
        "    sig = f\"_{MODEL_NAME}\"\n",
        "    if name.startswith(\"id_\") and sig in name:\n",
        "        core = name[len(\"id_\"):]\n",
        "        # 끝 확장자 제거\n",
        "        core = core.replace(\".json\", \"\")\n",
        "        return core  # \"<ver>_<MODEL_NAME>\"\n",
        "\n",
        "    return name  # 알 수 없으면 원본명(교집합에서 걸러짐)\n",
        "\n",
        "# 새 통일 형식(+확장자 없는 케이스)과 e5 형식 모두 검색\n",
        "map_paths = sorted(set(\n",
        "    glob.glob(str(EMB_DIR / f\"id_*_{MODEL_NAME}.json\")) +\n",
        "    glob.glob(str(EMB_DIR / f\"id_*_{MODEL_NAME}\")) +\n",
        "    glob.glob(str(EMB_DIR / \"id_mapping_torchdocs_*_e5_intfloat_e5_large_v2_mean.json\"))\n",
        "))\n",
        "map_by_key = {key_of(p): p for p in map_paths}\n",
        "\n",
        "print(\"\\n=== 샘플 순서 일치 확인 (각 버전 앞/뒤 2개) ===\")\n",
        "for vr in stats.get(\"version_ranges\", []):\n",
        "    ver, s, e = vr[\"version\"], vr[\"start\"], vr[\"end\"]\n",
        "    if ver not in map_by_key:\n",
        "        raise RuntimeError(f\"[매핑 파일 미발견] version='{ver}' 를 키로 갖는 id 파일을 찾지 못했습니다.\\n\"\n",
        "                           f\"- 검색한 경로: {EMB_DIR}\\n- MODEL_NAME='{MODEL_NAME}'\\n- 사용 가능한 키 예: {sorted(map_by_key.keys())[:5]} ...\")\n",
        "    with open(map_by_key[ver], encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    rows = data[\"id_mapping\"] if isinstance(data, dict) and \"id_mapping\" in data else data\n",
        "    if len(rows) == 0:\n",
        "        print(ver, \"빈 파일\")\n",
        "        continue\n",
        "\n",
        "    picks = [0, 1, max(0, len(rows)-2), len(rows)-1] if len(rows) > 3 else list(range(len(rows)))\n",
        "    ok_all = True\n",
        "    for i in picks:\n",
        "        gid = gids[s + i]\n",
        "        rid = rows[i][\"id\"] if isinstance(rows[i], dict) else rows[i]\n",
        "        ok = (gid == rid)\n",
        "        ok_all = ok_all & ok\n",
        "        print(f\"{ver}  idx={i:>5} | global[{s+i}] == rows[{i}] ? {ok}\")\n",
        "    print(\" -> version ok:\", ok_all)\n",
        "\n",
        "# === 4) 벡터 유효성(재구성 & L2 노름 ~ 1) 샘플 체크 ===\n",
        "try:\n",
        "    import numpy as _np\n",
        "    N = min(10, idx.ntotal)\n",
        "    ids = _np.random.choice(idx.ntotal, size=N, replace=False)\n",
        "    bad = 0\n",
        "    for i in ids:\n",
        "        v = _np.empty((idx.d,), dtype='float32')\n",
        "        idx.reconstruct(i, v)\n",
        "        norm = float(_np.linalg.norm(v))\n",
        "        if not (_np.isfinite(norm) and 0.99 <= norm <= 1.01):\n",
        "            bad += 1\n",
        "    print(\"\\n=== 재구성 벡터 정규화 검사 ===\")\n",
        "    print(f\"샘플 {N}개 중 정규화 범위(≈1) 벗어난 개수:\", bad)\n",
        "except Exception as e:\n",
        "    print(\"\\n(참고) 벡터 재구성 검사는 생략됨:\", e)\n",
        "\n",
        "# === 5) 글로벌 중복 id 존재 여부(정보용) ===\n",
        "cnt = Counter(gids)\n",
        "dup = [k for k, c in cnt.items() if c > 1]\n",
        "print(\"\\n=== global_ids 중복 id 개수 ===\")\n",
        "print(len(dup))\n",
        "if dup:\n",
        "    print(\"예시 5개:\", dup[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPPcrisaotLg",
        "outputId": "6912a273-b48e-44b5-e07b-991869f8a765"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 기본 정합성 ===\n",
            "ntotal == total_rows : True 38730 38730\n",
            "dim == stats.dimension: True 1024 1024\n",
            "len(global_ids) == total_rows: True 38730 38730\n",
            "\n",
            "=== version_ranges vs datasets.rows ===\n",
            "2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls | start=     0, end=  7901, rows=  7902 | span=  7902 | ok=True\n",
            "2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls | start=  7902, end= 16023, rows=  8122 | span=  8122 | ok=True\n",
            "2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls | start= 16024, end= 24116, rows=  8093 | span=  8093 | ok=True\n",
            "2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls | start= 24117, end= 32319, rows=  8203 | span=  8203 | ok=True\n",
            "2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls | start= 32320, end= 38729, rows=  6410 | span=  6410 | ok=True\n",
            "ALL_OK: True\n",
            "\n",
            "=== 샘플 순서 일치 확인 (각 버전 앞/뒤 2개) ===\n",
            "2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    0 | global[0] == rows[0] ? True\n",
            "2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    1 | global[1] == rows[1] ? True\n",
            "2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 7900 | global[7900] == rows[7900] ? True\n",
            "2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 7901 | global[7901] == rows[7901] ? True\n",
            " -> version ok: True\n",
            "2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    0 | global[7902] == rows[0] ? True\n",
            "2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    1 | global[7903] == rows[1] ? True\n",
            "2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 8120 | global[16022] == rows[8120] ? True\n",
            "2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 8121 | global[16023] == rows[8121] ? True\n",
            " -> version ok: True\n",
            "2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    0 | global[16024] == rows[0] ? True\n",
            "2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    1 | global[16025] == rows[1] ? True\n",
            "2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 8091 | global[24115] == rows[8091] ? True\n",
            "2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 8092 | global[24116] == rows[8092] ? True\n",
            " -> version ok: True\n",
            "2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    0 | global[24117] == rows[0] ? True\n",
            "2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    1 | global[24118] == rows[1] ? True\n",
            "2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 8201 | global[32318] == rows[8201] ? True\n",
            "2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 8202 | global[32319] == rows[8202] ? True\n",
            " -> version ok: True\n",
            "2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    0 | global[32320] == rows[0] ? True\n",
            "2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx=    1 | global[32321] == rows[1] ? True\n",
            "2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 6408 | global[38728] == rows[6408] ? True\n",
            "2.8_Snowflake_snowflake_arctic_embed_l_v2.0_cls  idx= 6409 | global[38729] == rows[6409] ? True\n",
            " -> version ok: True\n",
            "\n",
            "(참고) 벡터 재구성 검사는 생략됨: in method 'IndexFlat_reconstruct', argument 2 of type 'faiss::idx_t'\n",
            "\n",
            "=== global_ids 중복 id 개수 ===\n",
            "12\n",
            "예시 5개: ['300396de51e1f1034ce64f4e3a36b903f43bb985', '8907328a9ef69bc453f3f55e3fd90bd461e14678', 'f18b3bccedd55f8221497387b64325c2ebf306ed', 'be917dc4de73f230d926d09a98c776060e88f966', '2fcb44013a9391f52b9d6eaff0f66527d45a6630']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 중복 상세 확인"
      ],
      "metadata": {
        "id": "zAPJO9GCvped"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 중복 id 상세 & 벡터 동일성 확인 (출력만) ===\n",
        "# 단일 스크립트: 옛 e5(torchdocs) 형식과 새 통일형식(embeddings_<ver>_<MODEL>.npy / id_<ver>_<MODEL>) 모두 지원\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "import json, numpy as np, glob, os, faiss\n",
        "\n",
        "# ==== CONFIG ====\n",
        "BASE_DIR   = Path(\"/content/drive/MyDrive/Faiss/Snowflake_snowflake_arctic_embed_l_v2.0\")\n",
        "EMB_DIR    = BASE_DIR / \"embeddings\"\n",
        "INDEX_DIR  = BASE_DIR / \"index\"\n",
        "# 새 통일형식 사용 시 모델명 지정 (예: \"BAAI_bge_base_en_cls\", \"sentence_transformers_all_MiniLM_L6_v2_mean\", ...)\n",
        "MODEL_NAME = \"Snowflake_snowflake_arctic_embed_l_v2.0_cls\"\n",
        "\n",
        "# ==== 공통 유틸 ====\n",
        "def key_of(path_str: str) -> str:\n",
        "    \"\"\"\n",
        "    버전 키를 통일해서 반환.\n",
        "      - 옛 e5 형식: id_mapping_torchdocs_<key>_intfloat...json -> <key> (예: '2.4_chunks_e5')\n",
        "      - 새 통일형식: id_<ver>_<MODEL_NAME>[.json] -> '<ver>_<MODEL_NAME>'\n",
        "    \"\"\"\n",
        "    name = os.path.basename(path_str)\n",
        "\n",
        "    # (A) e5(torchdocs) 형식\n",
        "    if name.startswith(\"id_mapping_torchdocs_\") and \"_intfloat\" in name:\n",
        "        return name.split(\"id_mapping_torchdocs_\")[1].split(\"_intfloat\")[0]\n",
        "    if name.startswith(\"id_mapping_torchdocs_\") and \"_mean\" in name:\n",
        "        return name.split(\"id_mapping_torchdocs_\")[1].split(\"_mean\")[0]\n",
        "\n",
        "    # (B) 새 통일형식\n",
        "    if MODEL_NAME:\n",
        "        sig = f\"_{MODEL_NAME}\"\n",
        "        if name.startswith(\"id_\") and sig in name:\n",
        "            core = name[len(\"id_\"):]\n",
        "            core = core.replace(\".json\", \"\")\n",
        "            return core  # \"<ver>_<MODEL_NAME>\"\n",
        "\n",
        "    return name  # 알 수 없는 경우(교집합에서 걸러짐)\n",
        "\n",
        "def load_ids(map_path: str):\n",
        "    with open(map_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    id_list = data[\"id_mapping\"] if isinstance(data, dict) and \"id_mapping\" in data else data\n",
        "    if not isinstance(id_list, list) or not id_list or not isinstance(id_list[0], dict) or \"id\" not in id_list[0]:\n",
        "        raise ValueError(f\"[형식 오류] {map_path} : 'id' 키를 가진 dict 리스트가 필요합니다.\")\n",
        "    return id_list\n",
        "\n",
        "def vec_path_from_version_key(version_key: str) -> str:\n",
        "    \"\"\"\n",
        "    version_key에 맞는 임베딩 npy 경로를 생성.\n",
        "      - e5(torchdocs): '2.4_chunks_e5' -> embeddings_torchdocs_<key>_intfloat_e5_large_v2_mean.npy\n",
        "      - 새 통일형식:   '2.4_<MODEL_NAME>' -> embeddings_<ver>_<MODEL_NAME>.npy\n",
        "    \"\"\"\n",
        "    if MODEL_NAME and version_key.endswith(MODEL_NAME):\n",
        "        ver = version_key[:-(len(MODEL_NAME)+1)]  # 앞의 '<ver>_'만 추출\n",
        "        return str(EMB_DIR / f\"embeddings_{ver}_{MODEL_NAME}.npy\")\n",
        "    else:\n",
        "        # 옛 e5(torchdocs)\n",
        "        return str(EMB_DIR / f\"embeddings_torchdocs_{version_key}_intfloat_e5_large_v2_mean.npy\")\n",
        "\n",
        "def load_vec(version_key: str, local_idx: int) -> np.ndarray:\n",
        "    \"\"\"해당 버전의 로컬 인덱스 벡터 한 행을 정규화하여 반환.\"\"\"\n",
        "    npy_path = vec_path_from_version_key(version_key)\n",
        "    X = np.load(npy_path, mmap_mode=\"r\")  # (rows, dim)\n",
        "    v = X[local_idx].astype(\"float32\")\n",
        "    n = float(np.linalg.norm(v))\n",
        "    return (v / n) if n > 0 else v\n",
        "\n",
        "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.dot(a, b))  # a,b는 L2 정규화된 상태\n",
        "\n",
        "# ==== 데이터 로드 ====\n",
        "with open(INDEX_DIR / \"stats.json\", encoding=\"utf-8\") as f:\n",
        "    stats = json.load(f)\n",
        "with open(INDEX_DIR / \"global_ids.json\", encoding=\"utf-8\") as f:\n",
        "    gids = json.load(f)[\"ids\"]\n",
        "idx = faiss.read_index(str(INDEX_DIR / \"faiss.index\"))\n",
        "\n",
        "# id 매핑 파일 찾기(두 체계 모두 검색)\n",
        "map_paths = sorted(set(\n",
        "    glob.glob(str(EMB_DIR / \"id_mapping_torchdocs_*_e5_intfloat_e5_large_v2_mean.json\")) +\n",
        "    (glob.glob(str(EMB_DIR / f\"id_*_{MODEL_NAME}.json\")) + glob.glob(str(EMB_DIR / f\"id_*_{MODEL_NAME}\")) if MODEL_NAME else [])\n",
        "))\n",
        "map_by_key = {key_of(p): p for p in map_paths}\n",
        "\n",
        "# ==== 버전 범위 헬퍼 ====\n",
        "vranges = stats.get(\"version_ranges\", [])\n",
        "def which_version(global_idx: int):\n",
        "    for vr in vranges:\n",
        "        if vr[\"start\"] <= global_idx <= vr[\"end\"]:\n",
        "            return vr[\"version\"], global_idx - vr[\"start\"]\n",
        "    return \"unknown\", -1\n",
        "\n",
        "# ==== 1) 글로벌 중복 id 찾기 ====\n",
        "cnt = Counter(gids)\n",
        "dup_ids = [k for k, c in cnt.items() if c > 1]\n",
        "print(\"=== 중복 id 개수 ===\", len(dup_ids))\n",
        "\n",
        "# ==== 2) 각 중복 id의 등장 위치 수집 ====\n",
        "dup_locs = {}  # id -> [(version_key, global_idx, local_idx)]\n",
        "for did in dup_ids:\n",
        "    pos = [i for i, v in enumerate(gids) if v == did]\n",
        "    locs = []\n",
        "    for gi in pos:\n",
        "        ver, local = which_version(gi)\n",
        "        locs.append((ver, gi, local))\n",
        "    dup_locs[did] = sorted(locs, key=lambda x: (x[0], x[1]))\n",
        "\n",
        "# ==== 3) 벡터 동일성 확인 (코사인 ~ 1.0) ====\n",
        "print(\"\\n=== 중복 id 상세 (최대 12개 전부 표시) ===\")\n",
        "for did in dup_ids[:12]:\n",
        "    locs = dup_locs[did]\n",
        "    print(f\"\\n- id={did} (등장 {len(locs)}회)\")\n",
        "    for ver, gi, li in locs:\n",
        "        print(f\"  · {ver}: global[{gi}] / local[{li}]\")\n",
        "    # 벡터 비교\n",
        "    base_v = load_vec(locs[0][0], locs[0][2])\n",
        "    cos_all = []\n",
        "    for ver, gi, li in locs[1:]:\n",
        "        v = load_vec(ver, li)\n",
        "        cos_all.append(cosine(base_v, v))\n",
        "    if cos_all:\n",
        "        print(\"  코사인 유사도(첫 벡터 vs 나머지):\", [f\"{c:.6f}\" for c in cos_all])\n",
        "        all_one = all(abs(c-1.0) < 1e-6 for c in cos_all)\n",
        "        print(\"  → 모두 완전히 동일 벡터?\", all_one)\n",
        "    else:\n",
        "        print(\"  (중복 한 쌍만 존재)\")\n",
        "\n",
        "print(\"\\n=== 요약 ===\")\n",
        "only_within_version = all(len({v for v,_,_ in locs}) == 1 for locs in dup_locs.values())\n",
        "print(\"모든 중복이 같은 버전 내에서만 발생?\", only_within_version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp5y6qVYr1Ov",
        "outputId": "a0c246fb-8aa1-4cd0-a0f8-74b37bd096d7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 중복 id 개수 === 12\n",
            "\n",
            "=== 중복 id 상세 (최대 12개 전부 표시) ===\n",
            "\n",
            "- id=300396de51e1f1034ce64f4e3a36b903f43bb985 (등장 2회)\n",
            "  · 2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[113] / local[113]\n",
            "  · 2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[117] / local[117]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.463110']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=8907328a9ef69bc453f3f55e3fd90bd461e14678 (등장 2회)\n",
            "  · 2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[7451] / local[7451]\n",
            "  · 2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[7485] / local[7485]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.431984']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=f18b3bccedd55f8221497387b64325c2ebf306ed (등장 2회)\n",
            "  · 2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[7576] / local[7576]\n",
            "  · 2.4_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[7578] / local[7578]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.404683']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=be917dc4de73f230d926d09a98c776060e88f966 (등장 2회)\n",
            "  · 2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[8018] / local[116]\n",
            "  · 2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[8022] / local[120]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.463110']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=2fcb44013a9391f52b9d6eaff0f66527d45a6630 (등장 2회)\n",
            "  · 2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[15563] / local[7661]\n",
            "  · 2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[15597] / local[7695]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.431984']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=9e8805ba5e4bfa5839b81a278c477b39fe2f805b (등장 2회)\n",
            "  · 2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[15689] / local[7787]\n",
            "  · 2.5_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[15691] / local[7789]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.404683']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=3a3aa7be6555f6185e98d8866d8a6ca93d0defd0 (등장 2회)\n",
            "  · 2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[16143] / local[119]\n",
            "  · 2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[16147] / local[123]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.463110']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=06acf8db910e9ce4e1586b73ffb04462124bcc56 (등장 2회)\n",
            "  · 2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[23747] / local[7723]\n",
            "  · 2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[23781] / local[7757]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.431984']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=de86aa2f129c088e65b6cc62bdebe235fe6e27ef (등장 2회)\n",
            "  · 2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[23870] / local[7846]\n",
            "  · 2.6_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[23872] / local[7848]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.404683']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=83e92af1330a3767c1f4c9f6c3f4299c27298ceb (등장 2회)\n",
            "  · 2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[24237] / local[120]\n",
            "  · 2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[24241] / local[124]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.463110']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=27f0699f4edbf4360cb5254e6522db68e17e3a83 (등장 2회)\n",
            "  · 2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[31950] / local[7833]\n",
            "  · 2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[31984] / local[7867]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.431984']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "- id=ee7e6e38ff3b68d02a8adbd7ad8631bd8c34c136 (등장 2회)\n",
            "  · 2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[32073] / local[7956]\n",
            "  · 2.7_Snowflake_snowflake_arctic_embed_l_v2.0_cls: global[32075] / local[7958]\n",
            "  코사인 유사도(첫 벡터 vs 나머지): ['0.404683']\n",
            "  → 모두 완전히 동일 벡터? False\n",
            "\n",
            "=== 요약 ===\n",
            "모든 중복이 같은 버전 내에서만 발생? True\n"
          ]
        }
      ]
    }
  ]
}