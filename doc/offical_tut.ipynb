{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:37.409792Z",
     "start_time": "2025-08-18T11:02:35.895341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ],
   "id": "fcc648d4ffad1ec0",
   "outputs": [],
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:37.487552Z",
     "start_time": "2025-08-18T11:02:37.461058Z"
    }
   },
   "source": [
    "# chat model\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ],
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:37.572643Z",
     "start_time": "2025-08-18T11:02:37.539706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedding mode\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ],
   "id": "a1a8a649e61cca21",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:38.273023Z",
     "start_time": "2025-08-18T11:02:37.581153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Faiss vector store\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ],
   "id": "ff94da6b5e94233b",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:38.331725Z",
     "start_time": "2025-08-18T11:02:38.293666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# indexing\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ],
   "id": "4ce21faf4a41dfbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:38.356842Z",
     "start_time": "2025-08-18T11:02:38.351725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# splitting document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ],
   "id": "c317bebb85ce0d4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:40.172535Z",
     "start_time": "2025-08-18T11:02:38.366388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# storing document\n",
    "import uuid\n",
    "\n",
    "# Add a unique ID to each document's metadata\n",
    "for doc in all_splits:\n",
    "    doc.metadata[\"id\"] = str(uuid.uuid4())\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(f\"First 3 document IDs in vector store: {document_ids[:3]}\")"
   ],
   "id": "5a1887ee657b2954",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 document IDs in vector store: ['84a479b5-5a57-434e-b386-1e3ec9a69648', '65e282eb-3c46-41d9-a787-95cd90b10cbc', '2befd0c1-8025-497c-a7ed-61938faab880']\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:40.392292Z",
     "start_time": "2025-08-18T11:02:40.195370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ],
   "id": "297819eff404181",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:40.408820Z",
     "start_time": "2025-08-18T11:02:40.405797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define a new structure for our retrieved context\n",
    "class RetrievedDoc(TypedDict):\n",
    "    id: str\n",
    "    text: str\n",
    "    score: float\n",
    "\n",
    "# Update the State to use our new structure\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[RetrievedDoc]\n",
    "    answer: str"
   ],
   "id": "37661979e6574616",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:40.448464Z",
     "start_time": "2025-08-18T11:02:40.444967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    Retrieves documents using a two-step MMR approach to include scores.\n",
    "    \"\"\"\n",
    "    # Define MMR parameters from your original retriever\n",
    "    k = 5\n",
    "    fetch_k = 10\n",
    "    lambda_mult = 0.6\n",
    "\n",
    "    # 1. First, get a larger pool of documents with their similarity scores\n",
    "    # This is the candidate set for MMR.\n",
    "    candidate_docs_with_scores = vector_store.similarity_search_with_score(\n",
    "        state[\"question\"],\n",
    "        k=fetch_k\n",
    "    )\n",
    "\n",
    "    # Create a dictionary to map document content/ID to its score for easy lookup\n",
    "    score_map = {doc.metadata[\"id\"]: score for doc, score in candidate_docs_with_scores}\n",
    "\n",
    "    # 2. Now, run a separate MMR search to get the final, diverse set of documents\n",
    "    # This call does NOT return scores, but it returns the correct documents.\n",
    "    mmr_selected_docs = vector_store.max_marginal_relevance_search(\n",
    "        state[\"question\"],\n",
    "        k=k,\n",
    "        fetch_k=fetch_k,\n",
    "        lambda_mult=lambda_mult\n",
    "    )\n",
    "\n",
    "    # 3. Combine the results to format the final context\n",
    "    formatted_context = []\n",
    "    for doc in mmr_selected_docs:\n",
    "        # Look up the score from the map we created in step 1\n",
    "        score = score_map.get(doc.metadata[\"id\"], \"N/A\")\n",
    "        formatted_context.append({\n",
    "            \"id\": doc.metadata.get(\"id\", \"N/A\"),\n",
    "            \"text\": doc.page_content,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return {\"context\": formatted_context}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"\n",
    "    Generates an answer using the formatted context from the retrieve step.\n",
    "    \"\"\"\n",
    "    # Adapt to the new context structure (a list of dictionaries)\n",
    "    docs_content = \"\\n\\n\".join(doc[\"text\"] for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ],
   "id": "3460871a815f8de",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:40.465515Z",
     "start_time": "2025-08-18T11:02:40.462736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ],
   "id": "1710f02a3c4076c1",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:46.585990Z",
     "start_time": "2025-08-18T11:02:40.479234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(\"--- Retrieved Context ---\")\n",
    "for doc in result['context']:\n",
    "    print(f\"  ID: {doc['id']}\")\n",
    "    print(f\"  Score: {doc['score']:.4f}\")\n",
    "    print(f\"  Text: {doc['text'][:200]}...\\n\")\n",
    "print(\"-------------------------\\n\")\n",
    "print(f\"Final Answer: {result['answer']}\")"
   ],
   "id": "a8e586caabc2a761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Context ---\n",
      "  ID: ee7adb46-d9ae-49a7-a103-be8adce23d4b\n",
      "  Score: 1.0511\n",
      "  Text: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outl...\n",
      "\n",
      "  ID: b05603dd-4815-4a19-a01c-4d27fc4d1285\n",
      "  Score: 1.3123\n",
      "  Text: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field d...\n",
      "\n",
      "  ID: 752fefab-98d1-4801-8051-e43301876bfc\n",
      "  Score: 1.0795\n",
      "  Text: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s...\n",
      "\n",
      "  ID: c6476961-21c6-4f97-a84e-65ef18afcb79\n",
      "  Score: 1.5196\n",
      "  Text: \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, metho...\n",
      "\n",
      "  ID: c765bb21-3b4a-4b9d-8ff7-dcb1592e07dd\n",
      "  Score: 1.5415\n",
      "  Text: Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:...\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Final Answer: Task decomposition is the process of breaking down a complex task into smaller, manageable sub-tasks. This can be achieved through methods such as simple prompting, task-specific instructions, or human inputs. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by organizing reasoning steps systematically.\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:02:49.712229Z",
     "start_time": "2025-08-18T11:02:46.601959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ],
   "id": "5a46a20ba4a4d046",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [{'id': 'ee7adb46-d9ae-49a7-a103-be8adce23d4b', 'text': 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#', 'score': np.float32(1.0510883)}, {'id': 'b05603dd-4815-4a19-a01c-4d27fc4d1285', 'text': 'The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', 'score': np.float32(1.312258)}, {'id': '752fefab-98d1-4801-8051-e43301876bfc', 'text': 'Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.', 'score': np.float32(1.0794634)}, {'id': 'c6476961-21c6-4f97-a84e-65ef18afcb79', 'text': '\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully', 'score': np.float32(1.5195655)}, {'id': 'c765bb21-3b4a-4b9d-8ff7-dcb1592e07dd', 'text': 'Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', 'score': np.float32(1.5414906)}]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, manageable sub-tasks or steps. This can be achieved through various methods, such as simple prompting techniques, task-specific instructions, or human inputs. It enables better planning and organization, facilitating the successful execution of intricate tasks.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 148
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
