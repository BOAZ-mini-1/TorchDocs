import json
import numpy as np
import time
from typing import Optional, Dict, List
from pathlib import Path
import glob
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import uuid
from datetime import datetime

def generate_unique_id() -> str:
    """ê³ ìœ  ID ìƒì„±"""
    return str(uuid.uuid4())

# JSONL íŒŒì¼ì—ì„œ "text_for_embedding" í•„ë“œì™€ "id" í•„ë“œ ì¶”ì¶œ
def load_jsonl_text_and_ids(path, text_field="text_for_embedding", limit=0): ## ì‹¤í—˜ìš© limit ì„¤ì • (ì „ì²´ ì²˜ë¦¬ ì‹œ 0ìœ¼ë¡œ ì„¤ì •)
    texts = []
    ids = []
    
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f): # ë¼ì¸ë³„ json íŒŒì‹±
            obj = json.loads(line)
            
            # id í•„ë“œ ì¶”ì¶œ
            if "id" in obj:
                ids.append(obj["id"])
            else:
                ids.append(f"missing_id_{i}")  # idê°€ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ê¸°ë°˜ ì„ì‹œ id ìƒì„±
            
            # text_for_embedding í•„ë“œ ì¶”ì¶œ
            if text_field in obj:
                texts.append(obj[text_field])
            elif text_field in obj.get("metadata", {}):
                texts.append(obj["metadata"][text_field])
            else:
                texts.append("") # í•„ë“œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            
            if limit and len(texts) >= limit:
                break
    
    return texts, ids

# JSONL íŒŒì¼ì—ì„œ "text_for_embedding" í•„ë“œë§Œ ì¶”ì¶œ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)
def load_jsonl_text(path, field="text_for_embedding", limit=0): ## ì‹¤í—˜ìš© limit ì„¤ì • (ì „ì²´ ì²˜ë¦¬ ì‹œ 0ìœ¼ë¡œ ì„¤ì •)
    data = []
    
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f): # ë¼ì¸ë³„ json íŒŒì‹±
            obj = json.loads(line)
            if field in obj:
                data.append(obj[field]) # text_for_embedding í•„ë“œ ì¶”ì¶œ
            elif field in obj.get("metadata", {}):
                data.append(obj["metadata"][field]) # metadata í•„ë“œ ì¶”ì¶œ
            else:
                data.append("") # í•„ë“œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            if limit and len(data) >= limit:
                break
    return data

# ID ë§¤í•‘ JSON íŒŒì¼ ì €ì¥
def save_id_mapping(ids: List[str], output_file: str, input_file: str, model_name: str, pooling_strategy: str):
    """ì„ë² ë”©ê³¼ ì›ë³¸ IDì˜ ë§¤í•‘ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    mapping_data = {
        "metadata": {
            "input_file": input_file,
            "model_name": model_name,
            "pooling_strategy": pooling_strategy,
            "total_items": len(ids),
            "created_at": datetime.now().isoformat()
        },
        "id_mapping": [
            {"index": i, "id": id_val} 
            for i, id_val in enumerate(ids)
        ]
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(mapping_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ID ë§¤í•‘ ì €ì¥ ì™„ë£Œ: {output_file}")
    return output_file

# ì„ë² ë”© ëª¨ë¸ ì •ë³´ ì¶œë ¥
def get_model_info(model_name: str) -> Optional[Dict]:
    info = {"model_name": model_name}

    # Sentence-Transformersë¡œ ëª¨ë¸ ë¡œë”© ì‹œë„
    try:
        st_model = SentenceTransformer(model_name)
        dim = st_model.get_sentence_embedding_dimension()

        # ì§€ì›í•˜ëŠ” pooling ì „ëµ ì¶”ì¶œ (mean, cls, max)
        pooling = []
        for _, mod in st_model.named_modules():
            cls_name = mod.__class__.__name__.lower()
            if "pooling" in cls_name:
                if getattr(mod, "pooling_mode_mean_tokens", False): pooling.append("mean")
                if getattr(mod, "pooling_mode_cls_token", False):  pooling.append("cls")
                if getattr(mod, "pooling_mode_max_tokens", False):  pooling.append("max")

        # ìµœì  pooling ì „ëµ ì„ íƒ
        optimal_pooling = "mean"  # ê¸°ë³¸ê°’
        if "cls" in pooling:
            optimal_pooling = "cls"  # clsê°€ ìˆìœ¼ë©´ ìš°ì„  ì„ íƒ (BERT ê³„ì—´)
        elif "mean" in pooling:
            optimal_pooling = "mean"  # meanì´ ìˆìœ¼ë©´ ì„ íƒ
        elif "max" in pooling:
            optimal_pooling = "max"  # maxë§Œ ìˆìœ¼ë©´ ì„ íƒ

        info.update({
            "backend": "sentence-transformers",
            "embedding_dimension": dim,
            "max_seq_length": st_model.max_seq_length, # ìµœëŒ€ í† í° ìˆ˜
            "supported_pooling": pooling,
            "optimal_pooling": optimal_pooling,
            "padding_side": getattr(st_model.tokenizer, "padding_side", "right"),
        })

        print(f"ğŸ“Š ëª¨ë¸: {model_name}")
        print(f"   ë°±ì—”ë“œ: sentence-transformers")
        print(f"   ì„ë² ë”© ì°¨ì›: {dim}")
        print(f"   ìµœëŒ€ í† í° ìˆ˜: {st_model.max_seq_length}")
        print(f"   ì§€ì› Pooling: {', '.join(pooling) if pooling else 'ì—†ìŒ'}")
        print(f"   ìµœì  Pooling: {optimal_pooling}")
        print(f"   íŒ¨ë”© ë°©í–¥: {info['padding_side']}")
        return info

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜ {model_name}: {e}")
        return None
    
# ì„ë² ë”© ìƒì„±
def embed_texts(texts, model_name, batch_size=32, device="cpu", pooling_strategy=None):
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    model = SentenceTransformer(model_name, device=device)  # ëª¨ë¸ ë¡œë“œ
    
    # pooling ì „ëµ ê²°ì •
    if pooling_strategy is None:
        # ëª¨ë¸ ì •ë³´ì—ì„œ ìµœì  pooling ì „ëµ ê°€ì ¸ì˜¤ê¸°
        model_info = get_model_info(model_name)
        if model_info and 'optimal_pooling' in model_info:
            pooling_strategy = model_info['optimal_pooling']
            print(f"ğŸ¯ ìë™ ì„ íƒëœ pooling ì „ëµ: {pooling_strategy}")
        else:
            pooling_strategy = "mean"  # ê¸°ë³¸ê°’
            print(f"âš ï¸  ê¸°ë³¸ pooling ì „ëµ ì‚¬ìš©: {pooling_strategy}")
    
    print(f"ğŸ”§ ì‚¬ìš©í•  pooling ì „ëµ: {pooling_strategy}")
    
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc=f"ì„ë² ë”© ìƒì„± ì¤‘ ({model_name})"):
        batch = texts[i:i + batch_size]
        # ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
        valid_batch = [text for text in batch if text.strip()]
        if valid_batch:
            try:
                # pooling ì „ëµì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
                emb = model.encode(valid_batch, convert_to_numpy=True, show_progress_bar=False, 
                                 pooling_strategy=pooling_strategy)
            except Exception as e:
                print(f"âš ï¸  {pooling_strategy} pooling ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                # pooling ì „ëµ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¬ì‹œë„
                emb = model.encode(valid_batch, convert_to_numpy=True, show_progress_bar=False)
            
            # 0ë²¡í„° íŒ¨ë”© ì¶”ê°€
            if len(valid_batch) < len(batch):
                padding = np.zeros((len(batch) - len(valid_batch), emb.shape[1]), dtype=emb.dtype)
                emb = np.vstack([emb, padding])
            embeddings.append(emb)
        else:
            # ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ë¹ˆ ê²½ìš°
            emb = np.zeros((len(batch), model.get_sentence_embedding_dimension()), dtype=np.float32)
            embeddings.append(emb)
    
    return np.vstack(embeddings)  # ìµœì¢… ì„ë² ë”© ë°°ì—´ ë³€í™˜

# ì‹¤í—˜ ë¡œê·¸ ì €ì¥ (txt)
def save_experiment_log(results: List[Dict], output_dir: str, experiment_id: str):
    log_data = {
        'experiment_id': experiment_id,
        'timestamp': datetime.now().isoformat(),
        'total_files': len(results),
        'results': results
    }
    
    log_file = Path(output_dir) / f"experiment_log_{experiment_id}.txt"
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"ì‹¤í—˜ ID: {log_data['experiment_id']}\n")
        f.write(f"íƒ€ì„ìŠ¤íƒ¬í”„: {log_data['timestamp']}\n")
        f.write(f"ì „ì²´ íŒŒì¼ ìˆ˜: {log_data['total_files']}\n\n")
        
        for i, result in enumerate(log_data['results'], 1):
            f.write(f"=== ê²°ê³¼ {i} ===\n")
            for key, value in result.items():
                f.write(f"{key}: {value}\n")
            f.write("\n")
    
    print(f"ğŸ“ ì‹¤í—˜ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file}")
    return log_file


# ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰
def run_embedding_experiment(input_file, output_dir, model_name, batch_size=32, device="cpu", limit=0, pooling_strategy=None):
    start_time = time.time()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ (í…ìŠ¤íŠ¸ì™€ ID ëª¨ë‘ ì¶”ì¶œ)
    print(f"ğŸ“„ ë°ì´í„° ë¡œë”© ì¤‘: {input_file}")
    texts, ids = load_jsonl_text_and_ids(input_file, text_field="text_for_embedding", limit=limit)
    print(f"ğŸ“Š ë¡œë“œëœ í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}")
    print(f"ğŸ“Š ë¡œë“œëœ ID ìˆ˜: {len(ids)}")
    
    # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
    valid_texts = [text for text in texts if text.strip()]
    print(f"ğŸ“ ìœ íš¨í•œ í…ìŠ¤íŠ¸: {len(valid_texts)} / {len(texts)}")
    
    print(f"\nğŸ”¬ ì‹¤í—˜ ì‹œì‘: {model_name}")
    print("=" * 60)
    
    try:
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        model_info = get_model_info(model_name)
        if model_info is None:
            return None
        
        # pooling ì „ëµ ê²°ì •
        if pooling_strategy is None:
            pooling_strategy = model_info.get('optimal_pooling', 'mean')
        
        # ì„ë² ë”© ìƒì„±
        embeddings = embed_texts(texts, model_name, batch_size=batch_size, device=device, pooling_strategy=pooling_strategy)
        
        # ê²°ê³¼ ì €ì¥
        stem = Path(input_file).stem
        model_safe_name = model_name.replace("/", "_").replace("-", "_")
        
        # ì„ë² ë”© íŒŒì¼ ì €ì¥
        embedding_output_file = output_path / f"embeddings_{stem}_{model_safe_name}_{pooling_strategy}.npy"
        np.save(embedding_output_file, embeddings.astype(np.float32))
        print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {embedding_output_file} (shape={embeddings.shape})")
        
        # ID ë§¤í•‘ íŒŒì¼ ì €ì¥
        id_mapping_file = output_path / f"id_mapping_{stem}_{model_safe_name}_{pooling_strategy}.json"
        save_id_mapping(ids, str(id_mapping_file), str(input_file), model_name, pooling_strategy)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            "input_file": str(input_file),
            "model_name": model_name,
            "output_file": str(embedding_output_file),
            "id_mapping_file": str(id_mapping_file),
            "shape": embeddings.shape,
            "model_info": model_info,
            "pooling_strategy": pooling_strategy,
            "valid_texts": len(valid_texts),
            "total_ids": len(ids),
            "processing_time_seconds": processing_time,
            "batch_size": batch_size,
            "device": device,
            "limit": limit
        }
        
        print(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {model_name} - {result['shape']}")
        print(f"ğŸ”§ Pooling ì „ëµ: {pooling_strategy}")
        print(f"ğŸ“„ ID ë§¤í•‘: {len(ids)}ê°œ")
        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        
        return result
        
    except Exception as e:
        print(f"âŒ ì‹¤í—˜ ì˜¤ë¥˜ {model_name}: {e}")
        return {
            "input_file": str(input_file),
            "model_name": model_name, 
            "error": str(e),
            "processing_time_seconds": time.time() - start_time
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ TorchDocs ì„ë² ë”© ìƒì„± ì‹œì‘!")
    print("=" * 60)
    
    # ì„¤ì •
    input_dir = "TorchDocs/data/preprocessed/"
    output_dir = "TorchDocs/data/embeddings_output"
    model_name = "BAAI/bge-large-en"
    batch_size = 16
    device = "cpu"  ## GPU ì‚¬ìš© ì‹œ "cuda"ë¡œ ë³€ê²½
    limit = 10     ## í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œë§Œ ì²˜ë¦¬, ì „ì²´ ì²˜ë¦¬ ì‹œ 0ìœ¼ë¡œ ì„¤ì •
    pooling_strategy = None  # None: ìë™ ì„ íƒ ("mean", "cls", "max" ì„ íƒë„ ê°€ëŠ¥)
    
    # ëª¨ë¸ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    model_safe_name = model_name.replace("/", "_").replace("-", "_")
    model_output_dir = Path(output_dir) / model_safe_name
    model_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ ëª¨ë¸ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬: {model_output_dir}")
    
    # ì‹¤í—˜ ID ìƒì„±
    experiment_id = generate_unique_id()
    print(f"ğŸ”¬ ì‹¤í—˜ ID: {experiment_id}")
    
    # pooling ì „ëµ ì •ë³´ ì¶œë ¥
    if pooling_strategy:
        print(f"ğŸ”§ ì‚¬ìš©ì ì§€ì • Pooling: {pooling_strategy}")
    else:
        print(f"ğŸ¯ Pooling: ìë™ ì„ íƒ (ëª¨ë¸ë³„ ìµœì  ì „ëµ)")
    
    # í´ë” ë‚´ì˜ ëª¨ë“  JSONL íŒŒì¼ ê²€ìƒ‰
    input_files = glob.glob(str(Path(input_dir) / "*.jsonl"))
    print(f"ğŸ“ ë°œê²¬ëœ JSONL íŒŒì¼ ìˆ˜: {len(input_files)}")
    for file in input_files:
        print(f"  - {file}")
    
    if not input_files:
        print("âŒ ì²˜ë¦¬í•  JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸš€ ëª¨ë“  JSONL íŒŒì¼ì— ëŒ€í•œ ì„ë² ë”© ì‹¤í—˜ ì‹œì‘!")
    print(f"ğŸ“Š ëª¨ë¸: {model_name}")
    print(f"âš™ï¸  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ“ ì œí•œ: {'ì „ì²´' if limit == 0 else f'{limit}ê°œ'}")
    
    # ê° íŒŒì¼ì— ëŒ€í•´ ì„ë² ë”© ì‹¤í—˜ ì‹¤í–‰
    all_results = []
    total_start_time = time.time()
    
    for input_file in input_files:
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {input_file}")
        file_result = run_embedding_experiment(
            input_file=input_file,
            output_dir=str(model_output_dir),
            model_name=model_name,
            batch_size=batch_size,
            device=device,
            limit=limit,
            pooling_strategy=pooling_strategy
        )
        if file_result:
            all_results.append(file_result)
    
    total_time = time.time() - total_start_time
    
    print(f"\nâœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ê²°ê³¼ ìˆ˜: {len(all_results)}")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    # ì‹¤í—˜ ë¡œê·¸ ì €ì¥
    if all_results:
        log_file = save_experiment_log(all_results, str(model_output_dir), experiment_id)
        print(f"ğŸ“ ì‹¤í—˜ ìš”ì•½:")
        print(f"   ë¡œê·¸ íŒŒì¼: {log_file}")
        print(f"   ì„±ê³µí•œ ì‹¤í—˜: {len([r for r in all_results if 'error' not in r])}")
        print(f"   ì‹¤íŒ¨í•œ ì‹¤í—˜: {len([r for r in all_results if 'error' in r])}")
        
        # pooling ì „ëµë³„ ê²°ê³¼ ìš”ì•½
        pooling_results = {}
        for result in all_results:
            if 'error' not in result:
                strategy = result.get('pooling_strategy', 'unknown')
                if strategy not in pooling_results:
                    pooling_results[strategy] = 0
                pooling_results[strategy] += 1
        
        if pooling_results:
            print(f"ğŸ”§ Pooling ì „ëµë³„ ê²°ê³¼:")
            for strategy, count in pooling_results.items():
                print(f"   {strategy}: {count}ê°œ íŒŒì¼")
        
        # ID ë§¤í•‘ íŒŒì¼ ìƒì„± ìš”ì•½
        successful_results = [r for r in all_results if 'error' not in r]
        if successful_results:
            print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼:")
            for result in successful_results:
                print(f"   ì„ë² ë”©: {Path(result['output_file']).name}")
                print(f"   ID ë§¤í•‘: {Path(result['id_mapping_file']).name}")
                print(f"   ID ê°œìˆ˜: {result['total_ids']}ê°œ")
    
    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()