import json
import numpy as np
from typing import Dict, List, Optional
from pathlib import Path
import glob
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from datetime import datetime


def load_jsonl_text_and_ids(path, text_field="text_for_embedding"):
    """JSONL íŒŒì¼ì—ì„œ "text_for_embedding" í•„ë“œì™€ "id" í•„ë“œ ì¶”ì¶œ"""
    texts = []
    ids = []
    
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            obj = json.loads(line)
            
            # id í•„ë“œ ì¶”ì¶œ
            if "id" in obj:
                ids.append(obj["id"])
            else:
                ids.append(f"missing_id_{i}")
            
            # text_for_embedding í•„ë“œ ì¶”ì¶œ
            if text_field in obj:
                texts.append(obj[text_field])
            elif text_field in obj.get("metadata", {}):
                texts.append(obj["metadata"][text_field])
            else:
                texts.append("")
    
    return texts, ids


def get_model_max_length(model_name: str) -> int:
    """SentenceTransformer ëª¨ë¸ì˜ max_seq_length ê°€ì ¸ì˜¤ê¸°"""
    try:
        st_model = SentenceTransformer(model_name)
        max_length = st_model.max_seq_length
        print(f"ğŸ“ ëª¨ë¸ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_length}")
        return max_length
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ 512 ì‚¬ìš©: {e}")
        return 512


def check_token_length(texts: List[str], ids: List[str], model_name: str) -> Dict:
    """ëª¨ë“  í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ í™•ì¸ ë° ëª¨ë¸ ì œí•œ ì´ˆê³¼ ì—¬ë¶€ ë¶„ì„"""
    print(f"ğŸ” ëª¨ë“  í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ë¶„ì„ ì¤‘: {model_name}")
    
    try:
        # SentenceTransformer ëª¨ë¸ì—ì„œ ìµœëŒ€ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
        max_length = get_model_max_length(model_name)
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        token_counts = []
        problematic_items = []
        
        for i, (text, item_id) in enumerate(tqdm(zip(texts, ids), desc="í† í° ìˆ˜ ê³„ì‚° ì¤‘", total=len(texts))):
            if text.strip():
                tokens = tokenizer.encode(text, add_special_tokens=True)
                token_count = len(tokens)
                token_counts.append(token_count)
                
                if token_count > max_length:
                    problematic_items.append({
                        'index': i,
                        'id': item_id,
                        'text_preview': text[:200] + "..." if len(text) > 200 else text,
                        'token_count': token_count,
                        'max_length': max_length,
                        'excess_tokens': token_count - max_length
                    })
            else:
                token_counts.append(0)
        
        analysis = {
            'total_items': len(texts),
            'valid_items': len([t for t in texts if t.strip()]),
            'max_tokens': max(token_counts) if token_counts else 0,
            'avg_tokens': np.mean(token_counts) if token_counts else 0,
            'median_tokens': np.median(token_counts) if token_counts else 0,
            'items_exceeding_limit': len(problematic_items),
            'additional_chunking_needed': len(problematic_items) > 0,
            'problematic_items': problematic_items,
            'model_max_length': max_length
        }
        
        print(f"ğŸ“Š í† í° ë¶„ì„ ê²°ê³¼:")
        print(f"   ì´ ì•„ì´í…œ ìˆ˜: {analysis['total_items']}")
        print(f"   ìœ íš¨í•œ ì•„ì´í…œ: {analysis['valid_items']}")
        print(f"   ìµœëŒ€ í† í° ìˆ˜: {analysis['max_tokens']}")
        print(f"   í‰ê·  í† í° ìˆ˜: {analysis['avg_tokens']:.1f}")
        print(f"   ì¤‘ìœ„ê°’ í† í° ìˆ˜: {analysis['median_tokens']:.1f}")
        print(f"   ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: {analysis['model_max_length']}")
        print(f"   ì œí•œ ì´ˆê³¼ ì•„ì´í…œ: {analysis['items_exceeding_limit']}")
        print(f"   ì¶”ê°€ ì²­í‚¹ í•„ìš”: {'ì˜ˆ' if analysis['additional_chunking_needed'] else 'ì•„ë‹ˆì˜¤'}")
        
        if analysis['additional_chunking_needed']:
            print(f"âš ï¸  ëª¨ë¸ ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ì•„ì´í…œë“¤:")
            for item in problematic_items:
                print(f"   ID: {item['id']} | í† í°: {item['token_count']} (ì´ˆê³¼: {item['excess_tokens']})")
        
        return analysis
        
    except Exception as e:
        print(f"âŒ í† í° ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None


def save_token_analysis_log(analysis_results: List[Dict], output_dir: str, timestamp: str):
    """í† í° ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    log_data = {
        'timestamp': timestamp,
        'total_files': len(analysis_results),
        'analysis_results': analysis_results
    }
    
    log_file = Path(output_dir) / f"token_analysis_log_{timestamp.replace(':', '-').replace('.', '-')}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ í† í° ë¶„ì„ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file}")
    return log_file


def analyze_tokens_for_file(input_file: str, model_name: str) -> Dict:
    """ë‹¨ì¼ íŒŒì¼ì— ëŒ€í•œ ëª¨ë“  í…ìŠ¤íŠ¸ í† í° ë¶„ì„"""
    print(f"ğŸ“„ ëª¨ë“  í…ìŠ¤íŠ¸ í† í° ë¶„ì„ ì¤‘: {input_file}")
    
    try:
        # ë°ì´í„°ì™€ ID ë¡œë“œ
        texts, ids = load_jsonl_text_and_ids(input_file, text_field="text_for_embedding")
        print(f"ğŸ“Š ë¡œë“œëœ ì•„ì´í…œ ìˆ˜: {len(texts)}")
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_texts = [text for text in texts if text.strip()]
        print(f"ğŸ“ ìœ íš¨í•œ ì•„ì´í…œ: {len(valid_texts)} / {len(texts)}")
        
        # í† í° ë¶„ì„
        token_analysis = check_token_length(texts, ids, model_name)
        
        if token_analysis:
            result = {
                "input_file": str(input_file),
                "model_name": model_name,
                "token_analysis": token_analysis,
                "valid_items": len(valid_texts)
            }
            return result
        else:
            return {
                "input_file": str(input_file),
                "model_name": model_name,
                "error": "í…ìŠ¤íŠ¸ í† í° ë¶„ì„ ì‹¤íŒ¨"
            }
            
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {input_file}: {e}")
        return {
            "input_file": str(input_file),
            "model_name": model_name,
            "error": str(e)
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª¨ë“  í…ìŠ¤íŠ¸ì˜ í† í° ë¶„ì„ ì‹¤í–‰"""
    print("ğŸ” TorchDocs ì „ì²´ í…ìŠ¤íŠ¸ í† í° ë¶„ì„ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ“‹ ëª©ì : ëª¨ë“  text_for_embeddingì˜ í† í° ìˆ˜ í™•ì¸")
    print("ğŸ“‹ ëŒ€ìƒ: preprocessed í´ë” ë‚´ ëª¨ë“  JSONL íŒŒì¼ì˜ ëª¨ë“  í…ìŠ¤íŠ¸")
    print("ğŸ“‹ í™•ì¸ì‚¬í•­: í…ìŠ¤íŠ¸ê°€ ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ ì—¬ë¶€")
    print("ğŸ“‹ ê²°ê³¼: ë¬¸ì œ ì—†ìœ¼ë©´ make_embeddings.py ì‹¤í–‰, ë¬¸ì œ ìˆìœ¼ë©´ ì¶”ê°€ ì²­í‚¹ í•„ìš”")
    print("=" * 60)
    
    # ì„¤ì •
    input_dir = "TorchDocs/data/preprocessed/"
    output_dir = "TorchDocs/token_analysis_output"
    model_name = "BAAI/bge-large-en"
    
    # ë¶„ì„ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().isoformat()
    print(f"ğŸ”¬ ë¶„ì„ ì‹œì‘ ì‹œê°„: {timestamp}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # í´ë” ë‚´ì˜ ëª¨ë“  JSONL íŒŒì¼ ê²€ìƒ‰
    input_files = glob.glob(str(Path(input_dir) / "*.jsonl"))
    print(f"ğŸ“ ë°œê²¬ëœ JSONL íŒŒì¼ ìˆ˜: {len(input_files)}")
    for file in input_files:
        print(f"  - {file}")
    
    if not input_files:
        print("âŒ ë¶„ì„í•  JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ” ëª¨ë“  JSONL íŒŒì¼ì˜ ì „ì²´ í…ìŠ¤íŠ¸ í† í° ë¶„ì„ ì‹œì‘!")
    print(f"ğŸ“Š ëª¨ë¸: {model_name}")
    
    # ê° íŒŒì¼ì— ëŒ€í•´ í† í° ë¶„ì„ ì‹¤í–‰
    all_results = []
    total_problematic_items = []
    
    for input_file in input_files:
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {input_file}")
        file_result = analyze_tokens_for_file(
            input_file=input_file,
            model_name=model_name
        )
        if file_result:
            all_results.append(file_result)
            
            # ë¬¸ì œê°€ ìˆëŠ” ì•„ì´í…œë“¤ ìˆ˜ì§‘
            if 'token_analysis' in file_result and file_result['token_analysis']['problematic_items']:
                for item in file_result['token_analysis']['problematic_items']:
                    item['source_file'] = input_file
                    total_problematic_items.append(item)
    
    print(f"\nâœ… ëª¨ë“  íŒŒì¼ì˜ ì „ì²´ í…ìŠ¤íŠ¸ í† í° ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ê²°ê³¼ ìˆ˜: {len(all_results)}")
    
    # ë¶„ì„ ë¡œê·¸ ì €ì¥
    if all_results:
        log_file = save_token_analysis_log(all_results, output_dir, timestamp)
        print(f"ğŸ“ ë¶„ì„ ìš”ì•½:")
        print(f"   ë¡œê·¸ íŒŒì¼: {log_file}")
        print(f"   ì„±ê³µí•œ ë¶„ì„: {len([r for r in all_results if 'error' not in r])}")
        print(f"   ì‹¤íŒ¨í•œ ë¶„ì„: {len([r for r in all_results if 'error' in r])}")
        
        # ì¶”ê°€ ì²­í‚¹ í•„ìš”ì„± ìš”ì•½
        additional_chunking_needed_count = sum([
            1 for r in all_results 
            if 'token_analysis' in r and r['token_analysis']['additional_chunking_needed']
        ])
        print(f"   ì¶”ê°€ ì²­í‚¹ì´ í•„ìš”í•œ íŒŒì¼: {additional_chunking_needed_count}ê°œ")
        print(f"   ì „ì²´ ë¬¸ì œ ì•„ì´í…œ ìˆ˜: {len(total_problematic_items)}ê°œ")
        
        # ë¬¸ì œê°€ ìˆëŠ” ì•„ì´í…œë“¤ì˜ ìƒì„¸ ì •ë³´
        if total_problematic_items:
            print(f"\nâš ï¸  í† í° ì œí•œ ì´ˆê³¼ ì•„ì´í…œ ìƒì„¸ ì •ë³´:")
            for i, item in enumerate(total_problematic_items, 1):
                print(f"   {i}. íŒŒì¼: {item['source_file']}")
                print(f"      ID: {item['id']}")
                print(f"      í† í° ìˆ˜: {item['token_count']} (ì´ˆê³¼: {item['excess_tokens']})")
                print(f"      ë¯¸ë¦¬ë³´ê¸°: {item['text_preview']}")
                print()
        
        # ìµœì¢… ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­:")
        if len(total_problematic_items) == 0:
            print(f"   âœ… ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ëª¨ë¸ ì œí•œ ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
            print(f"   âœ… í˜„ì¬ ìƒíƒœë¡œ make_embeddings.pyë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"   âš ï¸  ì´ {len(total_problematic_items)}ê°œ ì•„ì´í…œì—ì„œ ëª¨ë¸ ì œí•œ ì´ˆê³¼ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"   âš ï¸  ì¶”ê°€ ì²­í‚¹ ê¸°ëŠ¥ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print(f"   âš ï¸  make_embeddings.py ì‹¤í–‰ ì „ì— ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ë” ì„¸ë°€í•œ ì²­í‚¹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print(f"   ğŸ“‹ ë¬¸ì œ ì•„ì´í…œë“¤ì˜ ìƒì„¸ ì •ë³´ëŠ” ìœ„ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
    
    print("\nğŸ‰ ì „ì²´ í…ìŠ¤íŠ¸ í† í° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
